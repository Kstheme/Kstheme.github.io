import{_ as l,V as i,W as t,X as n,Y as a,Z as e,a0 as o,E as r}from"./framework-7208760f.js";const p={},c=o(`<h1 id="week4-中文分词和tfidf特征应用" tabindex="-1"><a class="header-anchor" href="#week4-中文分词和tfidf特征应用" aria-hidden="true">#</a> week4 中文分词和tfidf特征应用</h1><h2 id="中文分词" tabindex="-1"><a class="header-anchor" href="#中文分词" aria-hidden="true">#</a> 中文分词</h2><h3 id="难点" tabindex="-1"><a class="header-anchor" href="#难点" aria-hidden="true">#</a> 难点</h3><ul><li>歧义切分： <ul><li>南京市长江大桥</li><li>欢迎新老师生前来就餐</li><li>无线电法国别研究</li><li>乒乓球拍卖完了</li></ul></li><li>新词/专有名词/改造词等： <ul><li>九漏鱼</li><li>活性位点、受体剪切位点</li><li>虽迟但到、十分然拒</li></ul></li></ul><h3 id="正向最大匹配" tabindex="-1"><a class="header-anchor" href="#正向最大匹配" aria-hidden="true">#</a> 正向最大匹配</h3><p>分词步骤：</p><ol><li>收集一个词表</li><li>对于一个待分词的字符串，从前向后寻找最长的，在此表中出现的词，在词边界做切分</li><li>从切分处重复步骤2，直到字符串末尾</li></ol><p>实现方式一：</p><ol><li>找出词表中最大词长度</li><li>从字符串开头选取最大词长度的窗口，检查窗口内的词是否在词表中</li><li>如果在词表中，在词边界处进行切分，之后移动到词边界处，重复步骤2</li><li>如果不在词表中，窗口右边界退一个字符，之后检查窗口词是否在词表中</li></ol><p>实现方式二（利用前缀字典）：</p><ol><li>从前向后进行查找</li><li>如果窗口内的词是一个词前缀则继续扩大窗口</li><li>如果窗口内的词不是一个词前缀，则记录已发现的词，并将窗口移动到词边界</li></ol><div class="language-json line-numbers-mode" data-ext="json"><pre class="language-json"><code><span class="token comment">// 0代表不是一个词，但是是词的前缀</span>
<span class="token comment">// 1代表是一个词</span>

<span class="token comment">// 词表：[北京，北京大学，北京大学生，大学生]</span>
<span class="token punctuation">{</span>
    <span class="token property">&quot;北&quot;</span><span class="token operator">:</span> <span class="token number">0</span><span class="token punctuation">,</span>
    <span class="token property">&quot;北京&quot;</span><span class="token operator">:</span> <span class="token number">1</span><span class="token punctuation">,</span>
    <span class="token property">&quot;北京大&quot;</span><span class="token operator">:</span> <span class="token number">0</span><span class="token punctuation">,</span>
    <span class="token property">&quot;北京大学&quot;</span><span class="token operator">:</span> <span class="token number">1</span><span class="token punctuation">,</span>
    <span class="token property">&quot;北京大学生&quot;</span><span class="token operator">:</span> <span class="token number">1</span><span class="token punctuation">,</span>
    <span class="token property">&quot;大&quot;</span><span class="token operator">:</span> <span class="token number">0</span><span class="token punctuation">,</span>
    <span class="token property">&quot;大学&quot;</span><span class="token operator">:</span> <span class="token number">0</span><span class="token punctuation">,</span>
    <span class="token property">&quot;大学生&quot;</span><span class="token operator">:</span> <span class="token number">1</span>
<span class="token punctuation">}</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>设：最大匹配值为5。从字符串的正方向出发，先截取前5个字符，与词典库中的词语进行对比。若比对<strong>不成功</strong>，则截取前4个字符进行对比，依次类推，直到仅剩第一个字符，自动进行截取，此次截取结束；若对比<strong>成功</strong>，则将该词语记录下来，并从句子中截取下来。直至句子全部被拆分为词语，以数组进行存储。</p><h3 id="逆向最大匹配算法" tabindex="-1"><a class="header-anchor" href="#逆向最大匹配算法" aria-hidden="true">#</a> 逆向最大匹配算法</h3><p>逆向最大匹配算法刚好与正向最大匹配算法相反，该算法旨在从句子末尾对句子进行分词操作，基本原理同正向最大匹配算法。</p><h3 id="双向最大匹配算法" tabindex="-1"><a class="header-anchor" href="#双向最大匹配算法" aria-hidden="true">#</a> 双向最大匹配算法</h3><p>双向最大匹配算法是同时采用<strong>正向最大匹配算法</strong>和<strong>逆向最大匹配算法</strong>，根据对比不同的执行结果，选择最优解。</p><p>有下述几种选择方案：</p><ol><li>如果分词数量结果不同：选择数量较少的那个。</li><li>如果分词数量结果相同： <ul><li>分词结果相同，返回任意一个</li><li>分词结果不同，返回单个字数较少的一个</li><li>若分词单个字数也相同，任意返回一个</li></ul></li></ol><p>正向最大切分，负向最大切分，双向最大切分共同的缺点：</p><ol><li>对词表极为依赖，如果没有词表，则无法进行；如果词表缺少需要的词，结果也不会正确</li><li>切分过程中不会关注整个句子表达的意思，只会将句子看成一个个片段</li><li>如果文本中出现一定的错别字，会造成一连串影响</li><li>对于人名等的无法枚举实体词无法有效的处理</li></ol><h3 id="jieba分词" tabindex="-1"><a class="header-anchor" href="#jieba分词" aria-hidden="true">#</a> jieba分词</h3><p>jieba分词分为三种模式：</p><ul><li><strong>精确模式</strong>：就是把一段文本精确地切分成若干个中文单词，若干个中文单词之间经过组合，就精确地还原为之前的文本。其中<strong>不存在冗余</strong>单词。</li><li><strong>全模式：<strong>将一段文本中所有可能的词语都扫描出来，可能有一段文本它可以切分成不同的模式，或者有不同的角度来切分变成不同的词语，在全模式下，Jieba库会将各种不同的组合都挖掘出来。分词后的信息再组合起来</strong>会有冗余</strong>，不再是原来的文本。</li><li>**搜索引擎模式：**在精确模式基础上，对发现的那些长的词语，我们会对它再次切分，进而适合搜索引擎对短词语的索引和搜索。<strong>也有冗余</strong>。</li></ul><h3 id="基于机器学习" tabindex="-1"><a class="header-anchor" href="#基于机器学习" aria-hidden="true">#</a> 基于机器学习</h3><ul><li>将问题转化为：对于句子中的每一个字，进行二分类判断，正类表示这句话中，它是词边界，负类表示它不是词边界</li><li>标注数据、训练模型，使模型可以完成上述判断，那么这个模型，可以称为一个分词模型</li><li>序列标注问题</li></ul><h3 id="关于分词" tabindex="-1"><a class="header-anchor" href="#关于分词" aria-hidden="true">#</a> 关于分词</h3><p>目前，对于中文分词的研究在逐渐减少，有以下几方面原因：</p><ol><li>目前的分词在由大部分情况下，效果已经比较理想，优化空间不大</li><li>分词即使发生错误，下游任务不是一定发生错误，所以不值得花大量精力优化分词</li><li>随着神经网络和预训练模型的兴起，中文任务逐渐不再需要分词，甚至不做分词，效果更好</li><li>解决不了的问题，是真的不好解决了</li></ol><h2 id="参考文献" tabindex="-1"><a class="header-anchor" href="#参考文献" aria-hidden="true">#</a> 参考文献</h2>`,30),d={href:"https://blog.csdn.net/weixin_43479947/article/details/126354084",target:"_blank",rel:"noopener noreferrer"},u={href:"https://blog.csdn.net/qq_45288176/article/details/115681292",target:"_blank",rel:"noopener noreferrer"};function h(k,b){const s=r("ExternalLinkIcon");return i(),t("div",null,[c,n("p",null,[a("[1] "),n("a",d,[a("分词算法----正向和逆向最大匹配算法(含Python代码实现)"),e(s)])]),n("p",null,[a("[2] "),n("a",u,[a("jieba分词的最详细解读"),e(s)])])])}const v=l(p,[["render",h],["__file","week4 中文分词和tfidf特征应用.html.vue"]]);export{v as default};
