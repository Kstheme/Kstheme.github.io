import{_ as s}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as a,e as t,o as e}from"./app-CRQr5f3f.js";const p={};function i(l,n){return e(),a("div",null,[...n[0]||(n[0]=[t(`<h1 id="the-three-levels-of-handcrafting-self-attention" tabindex="-1"><a class="header-anchor" href="#the-three-levels-of-handcrafting-self-attention"><span>The three levels of handcrafting self-attention</span></a></h1><h2 id="introduction" tabindex="-1"><a class="header-anchor" href="#introduction"><span>Introduction</span></a></h2><p>There are many details in the implementation process of self-attention, and different interviews have different requirements for the implementation of self-attention. So, we need to learn various ways to implement self-attention, so as to tell the interviewer that we understand the details of self-attention.</p><h2 id="the-formula-of-self-attention" tabindex="-1"><a class="header-anchor" href="#the-formula-of-self-attention"><span>The formula of self-attention</span></a></h2><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>Attention</mtext><mo stretchy="false">(</mo><mi>Q</mi><mo separator="true">,</mo><mi>K</mi><mo separator="true">,</mo><mi>V</mi><mo stretchy="false">)</mo><mo>=</mo><mtext>softmax</mtext><mo stretchy="false">(</mo><mfrac><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac><mo stretchy="false">)</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">\\text{Attention}(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">Attention</span></span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.4483em;vertical-align:-0.93em;"></span><span class="mord text"><span class="mord">softmax</span></span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5183em;"><span style="top:-2.2528em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8572em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.8172em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"></path></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1828em;"><span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.93em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span></span></p><h2 id="code-implementation" tabindex="-1"><a class="header-anchor" href="#code-implementation"><span>Code Implementation</span></a></h2><h3 id="first-realm-simplified-version" tabindex="-1"><a class="header-anchor" href="#first-realm-simplified-version"><span>First Realm: Simplified Version</span></a></h3><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py"><pre><code class="language-python"><span class="line"><span class="token keyword">import</span> math</span>
<span class="line"><span class="token keyword">import</span> torch</span>
<span class="line"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn</span>
<span class="line"></span>
<span class="line"><span class="token keyword">class</span> <span class="token class-name">SelfAttentionV1</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">:</span> <span class="token builtin">int</span> <span class="token operator">=</span> <span class="token number">728</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> <span class="token boolean">None</span><span class="token punctuation">:</span></span>
<span class="line">        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span></span>
<span class="line">        self<span class="token punctuation">.</span>hidden_dim <span class="token operator">=</span> hidden_dim</span>
<span class="line"></span>
<span class="line">        <span class="token comment"># Intialize three different linear application layers</span></span>
<span class="line">        self<span class="token punctuation">.</span>query_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">)</span></span>
<span class="line">        self<span class="token punctuation">.</span>key_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">)</span></span>
<span class="line">        self<span class="token punctuation">.</span>value_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line">    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span></span>
<span class="line">        <span class="token comment"># x shape is: (batch_size, seq_len, hidden_dim)</span></span>
<span class="line"></span>
<span class="line">        <span class="token comment"># acquire different Q, K, V</span></span>
<span class="line">        Q <span class="token operator">=</span> self<span class="token punctuation">.</span>query_proj<span class="token punctuation">(</span>x<span class="token punctuation">)</span></span>
<span class="line">        K <span class="token operator">=</span> self<span class="token punctuation">.</span>key_proj<span class="token punctuation">(</span>x<span class="token punctuation">)</span></span>
<span class="line">        V <span class="token operator">=</span> self<span class="token punctuation">.</span>value_proj<span class="token punctuation">(</span>x<span class="token punctuation">)</span></span>
<span class="line">        <span class="token comment"># Q, K, V shape: (batch_size, seq_len, hidden_dim)</span></span>
<span class="line"></span>
<span class="line">        <span class="token comment"># (batch_size, seq_len, hidden_dim) * (batch_size, hidden_dim, seq_len) = (batch_size, seq_len, seq_len)</span></span>
<span class="line">        attention_value <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span></span>
<span class="line">            Q<span class="token punctuation">,</span> K<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">)</span></span>
<span class="line">        <span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line">        <span class="token comment"># calculate attention weights</span></span>
<span class="line">        attention_weights <span class="token operator">=</span> torch<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>attention_value <span class="token operator">/</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_dim<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line">        <span class="token comment"># result of the calculation: (batch_size, seq_len, hidden_dim)</span></span>
<span class="line">        output <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>attention_weights<span class="token punctuation">,</span> V<span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line">        <span class="token keyword">return</span> output</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>The first realm is relatively simple, you can implement it entirely by following the formula.</p><h3 id="second-realm-efficiency-optimization" tabindex="-1"><a class="header-anchor" href="#second-realm-efficiency-optimization"><span>Second Realm: Efficiency Optimization</span></a></h3><p>Combine the Q, K, V martices and then split them.</p><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py"><pre><code class="language-python"><span class="line"><span class="token keyword">class</span> <span class="token class-name">SelfAttentionV2</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">)</span><span class="token punctuation">:</span></span>
<span class="line">        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span></span>
<span class="line">        self<span class="token punctuation">.</span>hidden_dim <span class="token operator">=</span> hidden_dim</span>
<span class="line">        self<span class="token punctuation">.</span>proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">,</span> hidden_dim <span class="token operator">*</span> <span class="token number">3</span><span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line">    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span></span>
<span class="line">        <span class="token comment"># x shape: (batch_size, seq_len, hidden_dim)</span></span>
<span class="line">        <span class="token comment"># QKV shape: (batch_size, seq_len, hidden_dim * 3)</span></span>
<span class="line">        QKV <span class="token operator">=</span> self<span class="token punctuation">.</span>proj<span class="token punctuation">(</span>x<span class="token punctuation">)</span></span>
<span class="line">        Q<span class="token punctuation">,</span> K<span class="token punctuation">,</span> V <span class="token operator">=</span> torch<span class="token punctuation">.</span>split<span class="token punctuation">(</span>QKV<span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_dim<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span></span>
<span class="line">        attention_weight <span class="token operator">=</span> torch<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span></span>
<span class="line">            torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>Q<span class="token punctuation">,</span> K<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">/</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_dim<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span></span>
<span class="line">        <span class="token punctuation">)</span></span>
<span class="line">        output <span class="token operator">=</span> attention_weight @ V</span>
<span class="line">        <span class="token keyword">return</span> output</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="third-realm-add-some-details-interview-style-implementation" tabindex="-1"><a class="header-anchor" href="#third-realm-add-some-details-interview-style-implementation"><span>Third Realm: add some details (interview-style implementation)</span></a></h3><p>In addition to the formula, there are some additional details:</p><ul><li>add dropout</li><li>given that each sentence has a distinct length, it is necessary to add an attention mask</li><li>output martix mapping</li></ul><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py"><pre><code class="language-python"><span class="line"><span class="token keyword">class</span> <span class="token class-name">SelfAttentionV3</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">,</span> dropout_rate<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> <span class="token boolean">None</span><span class="token punctuation">:</span></span>
<span class="line">        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span></span>
<span class="line">        self<span class="token punctuation">.</span>hidden_dim <span class="token operator">=</span> hidden_dim</span>
<span class="line"></span>
<span class="line">        self<span class="token punctuation">.</span>proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">,</span> hidden_dim <span class="token operator">*</span> <span class="token number">3</span><span class="token punctuation">)</span></span>
<span class="line">        self<span class="token punctuation">.</span>attention_dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>dropout_rate<span class="token punctuation">)</span></span>
<span class="line">        self<span class="token punctuation">.</span>output_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line">    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> attention_mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span></span>
<span class="line">        <span class="token comment"># x shape: (batch_size, seq_len, hidden_dim)</span></span>
<span class="line">        QKV <span class="token operator">=</span> self<span class="token punctuation">.</span>proj<span class="token punctuation">(</span>x<span class="token punctuation">)</span></span>
<span class="line">        Q<span class="token punctuation">,</span> K<span class="token punctuation">,</span> V <span class="token operator">=</span> torch<span class="token punctuation">.</span>split<span class="token punctuation">(</span>QKV<span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_dim<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line">        attention_weight <span class="token operator">=</span> Q @ K<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token operator">/</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_dim<span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line">        <span class="token comment"># if attention_mask is not None, we need to assign an extremely small value to the masked tokens —— this way, their value will be 0 after applying Softmax.</span></span>
<span class="line">        <span class="token keyword">if</span> attention_mask <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span></span>
<span class="line">            attention_weight <span class="token operator">=</span> attention_weight<span class="token punctuation">.</span>masked_fill<span class="token punctuation">(</span></span>
<span class="line">                attention_mask <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">,</span></span>
<span class="line">                <span class="token builtin">float</span><span class="token punctuation">(</span><span class="token string">&quot;1e-20&quot;</span><span class="token punctuation">)</span></span>
<span class="line">            <span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line">        attention_weight <span class="token operator">=</span> torch<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span></span>
<span class="line">            attention_weight<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span></span>
<span class="line">        <span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line">        <span class="token comment"># applying dropout</span></span>
<span class="line">        attention_weight <span class="token operator">=</span> self<span class="token punctuation">.</span>attention_dropout<span class="token punctuation">(</span>attention_weight<span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line">        attention_result <span class="token operator">=</span> attention_weight @ V</span>
<span class="line"></span>
<span class="line">        output <span class="token operator">=</span> self<span class="token punctuation">.</span>output_proj<span class="token punctuation">(</span>attention_result<span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line">        <span class="token keyword">return</span> output</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="the-core-optimization-context-iteration-logic-from-v1-to-v3" tabindex="-1"><a class="header-anchor" href="#the-core-optimization-context-iteration-logic-from-v1-to-v3"><span>The core optimization context (iteration logic) from V1 to V3</span></a></h2><ol><li><strong>Phase 1: Engineering efficiency optimization (V1 -&gt; V2)</strong><ul><li><strong>Optimization Point</strong>: Merge 3 separate linear layers into 1 combined linear layer, then split QKV matrix.</li><li><strong>Core Logic</strong>: Mathematically completely equivalent (only weight concatenation), but reduces kernel launch times and memory fragmentation, while improving hardware parallel efficiency (GPUs can better utilize batch matrix multiplication computing power)</li><li><strong>Value</strong>: Transition from a &quot;teaching-level redundant implementation&quot; to &quot;engineering-efficient implementation&quot; —— no performance loss, only efficiency improvement.</li></ul></li><li><strong>Phase 2: Functionlity completeness optimization (V2 -&gt; V3)</strong><ul><li><strong>Optimization Point 1</strong>: add attention_mask support</li><li><strong>Problem Solved</strong>: Adapt to pratical scenarios (batch padding in NLP, causal masking for generation tasks) and shield against interference from invalid positions.</li><li><strong>Optimization Point 2</strong>: add Dropout for attention weights</li><li><strong>Problem Solved</strong>: Regularization —— prevent the model from over-relying on a few key positions and alleviate overfitting.</li><li><strong>Optimization Point 3</strong>: Add output linear projection (output_proj).</li><li><strong>Problem Solved</strong>: Refine the feature aggregated by attention, enhance the model&#39;s representational power, and adapt to the stacking of deep networks.</li><li><strong>Value</strong>: Shift from &quot;effieciency-centric&quot; to &quot;production-ready industrial-grade functionality&quot;, covering key needs like batch training and better generalization.</li></ul></li></ol>`,18)])])}const r=s(p,[["render",i]]),u=JSON.parse(`{"path":"/posts/Large%20Language%20Model%20(LLM)/the-three-levels-of-handcrafting-self-attention.html","title":"The three levels of handcrafting self-attention","lang":"en-US","frontmatter":{"author":"Kstheme","date":"2025-11-10T00:00:00.000Z","category":["Machine Learning"],"tag":["attention","llm"],"description":"The three levels of handcrafting self-attention Introduction There are many details in the implementation process of self-attention, and different interviews have different requ...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"The three levels of handcrafting self-attention\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2025-11-10T00:00:00.000Z\\",\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Kstheme\\"}]}"],["meta",{"property":"og:url","content":"https://kstheme.github.io/posts/Large%20Language%20Model%20(LLM)/the-three-levels-of-handcrafting-self-attention.html"}],["meta",{"property":"og:site_name","content":"Kstheme's Blog"}],["meta",{"property":"og:title","content":"The three levels of handcrafting self-attention"}],["meta",{"property":"og:description","content":"The three levels of handcrafting self-attention Introduction There are many details in the implementation process of self-attention, and different interviews have different requ..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"article:author","content":"Kstheme"}],["meta",{"property":"article:tag","content":"llm"}],["meta",{"property":"article:tag","content":"attention"}],["meta",{"property":"article:published_time","content":"2025-11-10T00:00:00.000Z"}]]},"git":{},"readingTime":{"minutes":2.16,"words":648},"filePathRelative":"posts/Large Language Model (LLM)/the-three-levels-of-handcrafting-self-attention.md","excerpt":"\\n<h2>Introduction</h2>\\n<p>There are many details in the implementation process of self-attention, and different interviews have different requirements for the implementation of self-attention. So, we need to learn various ways to implement self-attention, so as to tell the interviewer that we understand the details of self-attention.</p>","autoDesc":true}`);export{r as comp,u as data};
